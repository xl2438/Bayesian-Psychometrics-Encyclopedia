
@article{johnson_using_2003,
	title = {Using {Data} {Augmentation} and {Markov} {Chain} {Monte} {Carlo} for the {Estimation} of {Unfolding} {Response} {Models}},
	volume = {28},
	issn = {1076-9986},
	doi = {10.3102/10769986028003195},
	abstract = {Unfolding response models, a class of item response theory (IRT) models that assume a unimodal item response function (IRF), are often used for the measurement of attitudes. Verhelst and Verstralen (1993)and Andrich and Luo (1993) independently developed unfolding response models by relating the observed responses to a more common monotone IRT model using a latent response model (LRM; Maris, 1995). This article generalizes their approach, and suggests a data augmentation scheme for the estimation of any unfolding response model. The article introduces two Markov chain Monte Carlo (MCMC) estimation procedures for the Bayesian estimation of unfolding model parameters; one is a direct implementation of MCMC, and the second utilizes the data augmentation method. We use the estimation procedure to analyze three data sets, one simulated, and two from real attitudinal surveys.},
	number = {3},
	urldate = {2016-06-29},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Johnson, Matthew S and Junker, Brian W},
	year = {2003},
	keywords = {data augmentation, latent response model, markov chain monte carlo, unfolding},
	pages = {195--230},
	file = {PDF:/home/xiang/Zotero/storage/5KZDJEDG/Johnson, Junker - 2003 - Using Data Augmentation and Markov Chain Monte Carlo for the Estimation of Unfolding Response Models.pdf:application/pdf;PDF:/home/xiang/Zotero/storage/GFWUL5H2/Johnson, Junker - 2003 - Using Data Augmentation and Markov Chain Monte Carlo for the Estimation of Unfolding Response Models(2).pdf:application/pdf},
}

@article{geman_stochastic_1984,
	title = {Stochastic {Relaxation}, {Gibbs} {Distributions}, and the {Bayesian} {Restoration} of {Images}},
	volume = {PAMI-6},
	issn = {01628828},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/22499653},
	doi = {10.1109/TPAMI.1984.4767596},
	abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
	number = {6},
	urldate = {2016-11-21},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Geman, Stuart and Geman, Donald},
	month = jun,
	year = {1984},
	keywords = {Annealing, Gibbs distribution, image restoration, line process, MAP estimate, Markov random field, relaxation scene modeling, spatial degradation},
	pages = {721--741},
}

@article{hastings_monte_1970,
	title = {Monte {Carlo} {Sampling} {Methods} {Using} {Markov} {Chains} and {Their} {Applications}},
	volume = {57},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2334940},
	abstract = {A generalization of the sampling method introduced by Metropolis et al. (1953) is presented along with an exposition of the relevant theory, techniques of application and methods and difficulties of assessing the error in Monte Carlo estimates. Examples of the methods, including the generation of random orthogonal matrices and potential applications of the methods to numerical problems arising in statistics, are discussed.},
	number = {1},
	journal = {Biometrika},
	author = {Hastings, W K},
	year = {1970},
	keywords = {and edward, arianna w, ation of state calculations, augusta h, by fast computing machines, marshall n, nicholas metropolis, rosenbluth, teller},
	pages = {97--109},
}

@article{conti_bayesian_2014,
	title = {Bayesian exploratory factor analysis},
	volume = {183},
	issn = {03044076},
	doi = {10.1016/j.jeconom.2014.06.008},
	abstract = {This paper develops and applies a Bayesian approach to Exploratory Factor Analysis that improves on ad hoc classical approaches. Our framework relies on dedicated factor models and simultaneously determines the number of factors, the allocation of each measurement to a unique factor, and the corresponding factor loadings. Classical identification criteria are applied and integrated into our Bayesian procedure to generate models that are stable and clearly interpretable. A Monte Carlo study confirms the validity of the approach. The method is used to produce interpretable low dimensional aggregates from a high dimensional set of psychological measurements.},
	number = {1},
	urldate = {2016-07-25},
	journal = {Journal of Econometrics},
	author = {Conti, Gabriella and Frühwirth-Schnatter, Sylvia and Heckman, James J. and Piatek, Rémi},
	year = {2014},
	keywords = {Bayesian factor models, Exploratory factor analysis, Identifiability, Marginal data augmentation, Model expansion, Model selection},
	pages = {31--57},
	file = {PDF:/home/xiang/Zotero/storage/28DYKC3Z/Conti et al. - 2014 - Bayesian exploratory factor analysis.pdf:application/pdf},
}

@article{rizopoulos_ltm_2015,
	title = {ltm : {An} {R} {Package} for {Latent} {Variable} {Modeling} and {Item} {Response} {Theory} {Analyses}},
	volume = {17},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v17/i05/},
	doi = {10.18637/jss.v017.i05},
	abstract = {The R package ltm has been developed for the analysis of multivariate dichotomous and polytomous data using latent variable models, under the Item Response Theory approach. For dichotomous data the Rasch, the Two-Parameter Logistic, and Birnbaum’s Three-Parameter models have been implemented, whereas for polytomous data Semejima’s Graded Response model is available. Parameter estimates are obtained under marginal maximum likelihood using the Gauss-Hermite quadrature rule. The capabilities and features of the package are illustrated using two real data examples},
	number = {5},
	urldate = {2019-05-28},
	journal = {Journal of Statistical Software},
	author = {Rizopoulos, Dimitris},
	month = nov,
	year = {2015},
	pages = {1--25},
}

@article{gelman_stan:_2015,
	title = {Stan: {A} {Probabilistic} {Programming} {Language} for {Bayesian} {Inference} and {Optimization}},
	volume = {40},
	abstract = {Stan is a free and open-source C++ program that performs Bayesian inference or optimiza- tion for arbitrary user-specified models and can be called from the command line, R, Python, Matlab, or Julia, and has great promise for fitting large and complex statistical models in many areas of application. We discuss Stan from users’ and developers’ perspectives and illustrate with a simple but nontrivial nonlinear regression example.},
	number = {5},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Gelman, A. and Lee, D. and Guo, J.},
	year = {2015},
	keywords = {bayesian inference, hierarchical models, probabilistic programming, statisti-},
	pages = {530--543},
}

@incollection{liu_estimating_2019,
	title = {Estimating {CDMs} {Using} {MCMC}},
	booktitle = {Handbook of {Diagnostic} {Classification} {Models}},
	publisher = {Springer},
	author = {Liu, Xiang and Johnson, Matthew S.},
	editor = {von Davier, Matthias and Lee, Young-Sun},
	year = {2019},
	pages = {629--646},
}

@article{fox_bayesian_2001,
	title = {Bayesian estimation of a multilevel {IRT} model using {Gibbs} sampling},
	volume = {66},
	issn = {00333123},
	doi = {10.1007/BF02294839},
	abstract = {In this article, a two-level regression model is imposed on the ability parameters in an item response theory (IRT) model. The advantage of using latent rather than observed scores as dependent variables of a multilevel model is that it offers the possibility of separating the influence of item difficulty and ability level and modeling response variation and measurement error. Another advantage is that, contrary to observed scores, latent scores are test-independent, which offers the possibility of using results from different tests in one analysis where the parameters of the IRT model and the multilevel model can be concurrently estimated. The two-parameter normal ogive model is used for the IRT measurement model. It will be shown that the parameters of the two-parameter normal ogive model and the multilevel model can be estimated in a Bayesian framework using Gibbs sampling. Examples using simulated and real data are given.},
	number = {2},
	journal = {Psychometrika},
	author = {Fox, Jean Paul and Glas, Cees A.W.},
	year = {2001},
}

@article{decarlo_recognizing_2012,
	title = {Recognizing {Uncertainty} in the {Q}-{Matrix} via a {Bayesian} {Extension} of the {DINA} {Model}},
	volume = {36},
	issn = {0146-6216},
	url = {http://journals.sagepub.com/doi/10.1177/0146621612449069},
	doi = {10.1177/0146621612449069},
	abstract = {In the typical application of a cognitive diagnosis model, the Q-matrix, which reflects the theory with respect to the skills indicated by the items, is assumed to be known. However, the Q-matrix is usually determined by expert judgment, and so there can be uncertainty about some of its elements. Here it is shown that this uncertainty can be recognized and explored via a Bayesian extension of the DINA (deterministic input noisy and) model. The approach used is to specify some elements of the Q-matrix as being random rather than as fixed; posterior distributions can then be used to obtain information about elements whose inclusion in the Q-matrix is questionable. Simulations show that this approach helps to recover the true Q-matrix when there is uncertainty about some elements. An application to the fraction-subtraction data of K. K. Tatsuoka suggests a modified Q-matrix that gives improved relative fit. © The Author(s) 2012.},
	number = {6},
	urldate = {2016-03-25},
	journal = {Applied Psychological Measurement},
	author = {DeCarlo, Lawrence T.},
	month = sep,
	year = {2012},
	pages = {447--468},
}

@article{chen_bayesian_2018,
	title = {Bayesian {Estimation} of the {DINA} {Q} matrix},
	volume = {83},
	issn = {00333123},
	url = {http://link.springer.com/10.1007/s11336-017-9579-4},
	doi = {10.1007/s11336-017-9579-4},
	abstract = {Cognitive diagnosis models are partially ordered latent class models and are used to classify students into skill mastery profiles. The deterministic inputs, noisy “and” gate model (DINA) is a popular psychometric model for cognitive diagnosis. Application of the DINA model requires content expert knowledge of a Q matrix, which maps the attributes or skills needed to master a collection of items. Misspecification of Q has been shown to yield biased diagnostic classifications. We propose a Bayesian framework for estimating the DINA Q matrix. The developed algorithm builds upon prior research (Chen, Liu, Xu, \& Ying, in J Am Stat Assoc 110(510):850–866, 2015) and ensures the estimated Q matrix is identified. Monte Carlo evidence is presented to support the accuracy of parameter recovery. The developed methodology is applied to Tatsuoka’s fraction-subtraction dataset.},
	number = {1},
	urldate = {2017-09-19},
	journal = {Psychometrika},
	author = {Chen, Yinghan and Culpepper, Steven Andrew and Chen, Yuguo and Douglas, Jeffrey},
	month = aug,
	year = {2018},
	keywords = {Bayesian statistics, cognitive diagnosis models, deterministic inputs, fraction-subtraction data, noisy “and” gate (DINA) model, Q matrix},
	pages = {89--108},
	file = {PDF:/home/xiang/Zotero/storage/LQIXDDCV/Chen et al. - 2017 - Bayesian Estimation of the DINA Q matrix.pdf:application/pdf},
}

@book{gelman_bayesian_2013,
	title = {Bayesian {Data} {Analysis}},
	isbn = {978-1-4398-4095-5},
	abstract = {Now in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied approach to analysis using up-to-date Bayesian methods. The authors—all leaders in the statistics community—introduce basic concepts from a data-analytic perspective before presenting advanced methods. Throughout the text, numerous worked examples drawn from real applications and research emphasize the use of Bayesian inference in practice. New to the Third Edition Four new chapters on nonparametric modeling Coverage of weakly informative priors and boundary-avoiding priors Updated discussion of cross-validation and predictive information criteria Improved convergence monitoring and effective sample size calculations for iterative simulation Presentations of Hamiltonian Monte Carlo, variational Bayes, and expectation propagation New and revised software code The book can be used in three different ways. For undergraduate students, it introduces Bayesian inference starting from first principles. For graduate students, the text presents effective current approaches to Bayesian modeling and computation in statistics and related fields. For researchers, it provides an assortment of Bayesian methods in applied statistics. Additional materials, including data sets used in the examples, solutions to selected exercises, and software instructions, are available on the book’s web page.},
	publisher = {Chapman and Hall/CRC},
	author = {Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson, David B and Vehtari, Aki and Rubin, Donald B},
	year = {2013},
}

@article{sinharay_assessing_2005,
	title = {Assessing {Fit} of {Unidimensional} {Item} {Response} {Theory} {Models} {Using} a {Bayesian} {Approach}},
	volume = {42},
	issn = {0022-0655},
	url = {http://doi.wiley.com/10.1111/j.1745-3984.2005.00021.x},
	doi = {10.1111/j.1745-3984.2005.00021.x},
	number = {4},
	urldate = {2017-10-09},
	journal = {Journal of Educational Measurement},
	author = {Sinharay, Sandip},
	month = dec,
	year = {2005},
	pages = {375--394},
	file = {PDF:/home/xiang/Zotero/storage/LBPTYERQ/Sinharay - 2005 - Assessing Fit of Unidimensional Item Response Theory Models Using a Bayesian Approach.pdf:application/pdf},
}

@article{sinharay_assessment_2015,
	title = {Assessment of {Person} {Fit} for {Mixed}-{Format} {Tests}},
	volume = {40},
	issn = {1076-9986},
	doi = {10.3102/1076998615589128},
	abstract = {Person-fit assessment may help the researcher to obtain additional information regarding the answering behavior of persons. Although several researchers examined person fit, there is a lack of research on person-fit assessment for mixed-format tests. In this article, the lz statistic and the \{zeta\}2 statistic, both of which have been used for tests with only dichotomous items or with only polytomous items, were modified for use with mixed-format tests. In a detailed simulation, the lz and \{zeta\}2 statistics are found to be conservative under a (frequentist) asymptotic normal approximation. However, the use of the statistics along with the (Bayesian) posterior predictive model checking method leads to a larger power. The suggested approaches are applied to an operational data set. The approaches appear to be satisfactory tools for assessing person fit for mixed-format tests.},
	number = {4},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Sinharay, S.},
	year = {2015},
	pages = {343--365},
	file = {PDF:/home/xiang/Zotero/storage/IRQNBBV5/Sinharay - 2016 - Assessment of Person Fit Using Resampling-Based Approaches.pdf:application/pdf},
}

@article{culpepper_bayesian_2015,
	title = {Bayesian {Estimation} of the {DINA} {Model} {With} {Gibbs} {Sampling}},
	volume = {40},
	issn = {1076-9986},
	url = {http://jeb.sagepub.com/cgi/doi/10.3102/1076998615595403},
	doi = {10.3102/1076998615595403},
	abstract = {A Bayesian model formulation of the deterministic inputs, noisy ''and'' gate (DINA) model is presented. Gibbs sampling is employed to simulate from the joint posterior distribution of item guessing and slipping parameters, subject attribute parameters, and latent class probabilities. The procedure extends concepts in Be´guin and Glas, Culpepper, and Sahu for estimating the guessing and slipping parameters in the three-and four-parameter normal-ogive models. The ability of the model to recover parameters is demonstrated in a simulation study. The technique is applied to a mental rotation test. The algorithm and vignettes are freely available to researchers as the ''dina'' R package.},
	number = {5},
	urldate = {2017-03-30},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Culpepper, Steven Andrew},
	month = oct,
	year = {2015},
	keywords = {bayesian statistics, Bayesian statistics, cognitive diagnosis, markov chain monte carlo, Markov chain Monte Carlo, spatial cognition},
	pages = {454--476},
	file = {PDF:/home/xiang/Zotero/storage/NXEF3FTA/Culpepper - 2015 - Bayesian Estimation of the DINA Model With Gibbs Sampling(2).pdf:application/pdf},
}

@article{maris_gibbs_2015,
	title = {A {Gibbs} {Sampler} for the ({Extended}) {Marginal} {Rasch} {Model}.},
	volume = {80},
	issn = {1860-0980},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/26493183},
	doi = {10.1007/s11336-015-9479-4},
	abstract = {In their seminal work on characterizing the manifest probabilities of latent trait models, Cressie and Holland give a theoretically important characterization of the marginal Rasch model. Because their representation of the marginal Rasch model does not involve any latent trait, nor any specific distribution of a latent trait, it opens up the possibility for constructing a Markov chain - Monte Carlo method for Bayesian inference for the marginal Rasch model that does not rely on data augmentation. Such an approach would be highly efficient as its computational cost does not depend on the number of respondents, which makes it suitable for large-scale educational measurement. In this paper, such an approach will be developed and its operating characteristics illustrated with simulated data.},
	number = {4},
	urldate = {2017-04-20},
	journal = {Psychometrika},
	author = {Maris, Gunter and Bechger, Timo and San Martin, Ernesto},
	month = dec,
	year = {2015},
	pages = {859--79},
	file = {PDF:/home/xiang/Zotero/storage/N47B9A29/Maris, Bechger, San Martin - 2015 - A Gibbs Sampler for the (Extended) Marginal Rasch Model.pdf:application/pdf},
}

@article{levy_rise_2009,
	title = {The {Rise} of {Markov} {Chain} {Monte} {Carlo} {Estimation} for {Psychometric} {Modeling}},
	volume = {2009},
	issn = {1687-952X},
	url = {http://www.hindawi.com/journals/jps/2009/537139/},
	doi = {10.1155/2009/537139},
	abstract = {Markov chain Monte Carlo (MCMC) estimation strategies represent a powerful approach to estimation in psychometric models. Popular MCMC samplers and their alignment with Bayesian approaches to modeling are discussed. Key historical and current developments of MCMC are surveyed, emphasizing how MCMC allows the researcher to overcome the limitations of other estimation paradigms, facilitates the estimation of models that might otherwise be intractable, and frees the researcher from certain possible misconceptions about the models.},
	urldate = {2016-08-01},
	journal = {Journal of Probability and Statistics},
	author = {Levy, Roy},
	year = {2009},
	pages = {1--18},
	file = {PDF:/home/xiang/Zotero/storage/QDI8ULLH/Levy - 2009 - The Rise of Markov Chain Monte Carlo Estimation for Psychometric Modeling.pdf:application/pdf},
}

@article{sinharay_assessing_2007,
	title = {Assessing {Fit} of {Cognitive} {Diagnostic} {Models} {A} {Case} {Study}},
	volume = {67},
	issn = {0013-1644},
	url = {http://journals.sagepub.com/doi/10.1177/0013164406292025},
	doi = {10.1177/0013164406292025},
	abstract = {A cognitive diagnostic model uses information from educational experts to describe the relationships between item performances and posited proficiencies. When the cognitive relationships can be described using a fully Bayesian model, Bayesian model checking procedures become available. Checking models tied to cognitive theory of the domains provides feedback to educators about the underlying cognitive theory. This article suggests a number of graphics and statistics for diagnosing problems with cognitive diagnostic models expressed as Bayesian networks. The suggested diagnostics allow the authors to identify the inadequacy of an earlier cognitive diagnostic model and to hypothesize an improved model that provides better fit to the data. (Contains 4 figures and 3 tables.)},
	number = {2},
	urldate = {2017-10-06},
	journal = {Educational and Psychological Measurement},
	author = {Sinharay, Sandip and Almond, Russell G},
	year = {2007},
	keywords = {Bayesian network, Bayesian residual, DIC, item fit, Markov chain Monte Carlo},
	pages = {239--257},
	file = {PDF:/home/xiang/Zotero/storage/MBFYVY34/Sinharay, Almond - 2007 - Assessing Fit of Cognitive Diagnostic Models A Case Study.pdf:application/pdf},
}

@article{neal_probabilistic_1998,
	title = {Probabilistic {Inference} {Using} {Markov} {Chain} {Monte} {Carlo} {Methods}},
	volume = {1},
	issn = {15206025},
	url = {papers2://publication/uuid/0C88167E-5379-4E4E-A9E4-007ABA4F716D},
	doi = {10.1021/np100920q},
	abstract = {Probabilistic inference is an attractive approach to uncertain reasoning and empirical learning in artificial intelligence.  Computational difficulties arise, however, because probabilistic models with the necessary realism and flexibility lead to complex distributions over high-dimensional spaces.  Related problems in other fields have been tackled using Monte Carlo methods based on sampling using Markov chains, providing a rich array of techniques that can be applied to problems in artificial intelligence.  The "Metropolis algorithm" has been used to solve difficult problems in statistical physics for over forty years, and, in the last few years, the related method of "Gibbs sampling" has been applied to the problems of statistical inference.  Concurrently, an alternative method for solving problems in statistical physics by means of dynamical simulations has been developed as well, and has recently been unified with the Metropolis algorithm to produce the "hybrid Monte Carlo" method.  In computer science, Markov chain sampling is the basis of the heuristic optimization technique of "simulated annealing", and has recently been used in randomized algorithms for approximate counting of large sets.  In this review, I outline the role of probabilistic inference in artificial intelligence, present the theory of Markov chains, and describe various Markov chain Monte Carlo algorithms, along with a number of supporting techniques.  I try to present a comprehensive picture of the range of methods that have been developed, including techniques from the varied literature that have not yet seen wide application in artificial intelligence, but which appear relevant.  As illustrative examples, I use the problems of probabilistic inference in expert systems, discovery of latent classes from data, and Bayesian learning for neural networks.},
	journal = {Technical Report},
	author = {Neal, Radford M},
	year = {1998},
	pages = {1--144},
}

@article{rubin_bayesianly_1984,
	title = {Bayesianly {Justifiable} and {Relevant} {Frequency} {Calculations} for the {Applied} {Statistician}},
	volume = {12},
	issn = {0090-5364},
	url = {http://projecteuclid.org/euclid.aos/1176346785},
	doi = {10.1214/aos/1176346785},
	abstract = {A common reaction among applied statisticians is that the Bayesian statistician's energies in an applied problem must be directed at the a priori elicitation of one model specification from which an optimal design and all inferences follow automatically by applying Bayes's theorem to calculate conditional distributions of unknowns given knowns. I feel, however, that the applied Bayesian statistician's tool-kit should be more extensive and include tools that may be usefully labeled frequency calculations. Three types of Bayesianly justifiable and relevant frequency calculations are presented using examples to convey their use for the applied statistician.},
	number = {4},
	journal = {The Annals of Statistics},
	author = {Rubin, Donald B.},
	year = {1984},
	pages = {1151--1172},
}

@article{cowles_markov_1996,
	title = {Markov {Chain} {Monte} {Carlo} {Convergence} {Diagnostics}: {A} {Comparative} {Review}},
	doi = {10.1080/01621459.1996.10476956},
	abstract = {A critical issue for users of Markov chain Monte Carlo (MCMC) methods in applications is how to determine when it is safe to stop sampling and use the samples to estimate characteristics of the distribution of interest. Research into methods of computing theoretical convergence bounds holds promise for the future but to date has yielded relatively little of practical use in applied work. Consequently, most MCMC users address the convergence problem by applying diagnostic tools to the output produced by running their samplers. After giving a brief overview of the area, we provide an expository review of 13 convergence diagnostics, describing the theoretical basis and practical implementation of each. We then compare their performance in two simple models and conclude that all of the methods can fail to detect the sorts of convergence failure that they were designed to identify. We thus recommend a combination of strategies aimed at evaluating and accelerating MCMC sampler convergence, including applying diagnostic procedures to a small number of parallel chains, monitoring autocorrelations and cross-correlations, and modifying parametrizations or sampling algorithms appropriately. We emphasize, however, that it is not possible to say with certainty that a finite sample from an MCMC algorithm is representative of an underlying stationary distribution.},
	journal = {Journal of the American Statistical Association},
	author = {Cowles, Mary Kathryn and Carlin, Bradley P.},
	year = {1996},
	keywords = {Autocorrelation, Gibbs sampler, Metropolis-Hastings algorithm},
}

@article{gelman_inference_1992,
	title = {Inference from {Iterative} {Simulation} {Using} {Multiple} {Sequences}},
	issn = {0883-4237},
	doi = {10.1214/ss/1177011136},
	abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were contin- ued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normal- ity after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random- effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
	journal = {Statistical Science},
	author = {Gelman, Andrew and Rubin, Donald B.},
	year = {1992},
}

@article{bock_fitting_1970,
	title = {Fitting a response model for n dichotomously scored items},
	volume = {35},
	issn = {00333123},
	url = {http://link.springer.com/10.1007/BF02291262},
	doi = {10.1007/BF02291262},
	abstract = {A method of estimating the parameters of the normal ogive model for diehotomously scored item-responses by maximum likelihood is demon- strated. Although the procedure requires numerical integration in order to evaluate the likelihood equations, a computer implemented Newton-l{\textasciitilde}aphson solution is shown to be straightforward in other respects. Empirical tests of the procedure show that the resulting estimates are very similar to those based on a conventional analysis of item "difficulties" and first factor load- ings obtained from the matrix of tetrachoric correlation coefficients. Problems of testing the fit of the model, and of obtaining invariant parameters are discussed.},
	number = {2},
	urldate = {2016-03-14},
	journal = {Psychometrika},
	author = {Bock, Darrell R. and Lieberman, Marcus},
	month = jun,
	year = {1970},
	pages = {179--197},
}

@article{ozturk_bayesian_2017,
	title = {A {Bayesian} {Robust} {IRT} {Outlier}-{Detection} {Model}},
	volume = {41},
	issn = {15523497},
	url = {http://journals.sagepub.com/doi/10.1177/0146621616679394},
	doi = {10.1177/0146621616679394},
	abstract = {In psychometric practice, the parameter estimates of a standard item-response theory (IRT) model can become biased when item-response data, of persons’ individual responses to test items, contain outliers relative to the model. Also, the manual removal of outliers can be a time-consuming and difficult task. Besides, removing outliers leads to data information loss in parameter estimation. To address these concerns, a Bayesian IRT model that includes person and latent item-response outlier parameters, in addition to person ability and item parameters, is proposed and illustrated, and is defined by item characteristic curves (ICCs) that are each specified by a robust, Student’s t-distribution function. The outlier parameters and the robust ICCs enable the model to automatically identify item-response outliers, and to make estimates of the person ability and item parameters more robust to outliers. Hence, under this IRT model, it is unnecessary to remove outliers from the data analysis. Our IRT model is illu...},
	number = {3},
	urldate = {2018-06-20},
	journal = {Applied Psychological Measurement},
	author = {Öztürk, Nicole K. and Karabatsos, George},
	month = may,
	year = {2017},
	keywords = {dichotomous items, item-response theory, misfit, polytomous items},
	pages = {195--208},
	file = {PDF:/home/xiang/Zotero/storage/288UJJU3/0146621616679394.pdf:application/pdf},
}

@phdthesis{liu_three_2019,
	type = {Doctoral {Dissertation}},
	title = {Three {Contributions} to {Latent} {Variable} {Modeling}},
	school = {Columbia University},
	author = {Liu, Xiang},
	year = {2019},
}
